{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy8HXbLYe-zc"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors (Original TensorFlow Version)\n",
        "##### Translated to PyTorch 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q0_10N6De-zh"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiR0ETqae-zt"
      },
      "source": [
        "# Introduction to Word Embeddings (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5d6ZHae-zz"
      },
      "source": [
        "This tutorial shows how to train a sentiment classifier on the IMDB dataset using learned word embeddings with PyTorch.\n",
        "\n",
        "First, here's a bit of background. Before we can build a model to predict the sentiment of a review, first we will need a way to represent the words of the review as numbers, so they can be processed by our network. There are several strategies to convert words to numbers.\n",
        "\n",
        "As a first attempt, we might one-hot encode each word. One problem with this approach is efficiency. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in our vocabulary. To one-hot encode each one, we would create a vector where 99.99% of the elements are zero!\n",
        "\n",
        "Instead, we can encode each word using a unique number. For example, we might assign 1 to 'the', 42 to 'dog', and 96 to 'cat', and so on. Using these numbers, we could encode a sentence like \"The dog and cat sat on the mat\" as [1, 42, 96, ...]. One problem still remains. Although we know dogs and cats are related, our representation doesn't encode that information for the classifier (the numbers 42 and 96 were arbitrarily chosen).\n",
        "\n",
        "Unlike the above methods, a word embedding is learned from data. An embedding represents each word as a n-dimensional vector of floating point values. These values are trainable parameters, weights learned while training the model. After training, we hope that similar words will be close together in the embedding space. We can visualize the learned embeddings by projecting them down to a 2- or 3-dimensional space.\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (e.g. sentiment classification). In this case, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
        "\n",
        "* Load word embeddings into your model that were pre-computed using a different machine learning task than the one you are trying to solve. These are called \"pre-trained word embeddings\".\n",
        "\n",
        "Here, we will take the first approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7zQoILT9e-z2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6MReNwJlOHJ"
      },
      "source": [
        "# Download the IMDB dataset\n",
        "\n",
        "The IMDB dataset comes packaged with TensorFlow/Keras. It has already been preprocessed such that the reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary. We'll use the Keras datasets to load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pxf4Qu3xe-0E"
      },
      "outputs": [],
      "source": [
        "# Install tensorflow for dataset loading only\n",
        "# !pip install tensorflow -q\n",
        "\n",
        "from tensorflow import keras\n",
        "imdb = keras.datasets.imdb\n",
        "\n",
        "# Number of words to consider as features\n",
        "num_words = 20000\n",
        "\n",
        "# load IMDB dataset as lists of integers\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGBirQQ2l1h7"
      },
      "source": [
        "The argument num_words=20000 keeps the top 20,000 most frequently occurring words in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GDRePL1flhhw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training examples: 25000, labels: 25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Training examples: {}, labels: {}\".format(len(train_data), len(train_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdVN8hYGloP7"
      },
      "source": [
        "The text of reviews have been converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gO0nNEZklltp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxGWS8mWlrGh"
      },
      "source": [
        "Movie reviews may be different lengths. The below code shows the number of words in the first and second reviews. Since inputs to a neural network must be the same length, we'll need to resolve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x6Bbi27-ltzb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(218, 189)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLcQxHtmmHBb"
      },
      "source": [
        "# Preprocess Data\n",
        "We will pad the arrays so they all have the same length. In PyTorch, we'll use `torch.nn.utils.rnn.pad_sequence`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A2JwvoSve-0H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 500)\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Cut texts after this number of words\n",
        "max_len = 500\n",
        "\n",
        "# Pad sequences to max_len\n",
        "def pad_sequences_pytorch(sequences, maxlen, padding='pre', truncating='pre', value=0):\n",
        "    \"\"\"Pad sequences to same length (PyTorch implementation)\"\"\"\n",
        "    result = np.zeros((len(sequences), maxlen), dtype=np.int64)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        if truncating == 'pre':\n",
        "            trunc = seq[-maxlen:]\n",
        "        else:\n",
        "            trunc = seq[:maxlen]\n",
        "        \n",
        "        if padding == 'pre':\n",
        "            result[i, -len(trunc):] = trunc\n",
        "        else:\n",
        "            result[i, :len(trunc)] = trunc\n",
        "    return result\n",
        "\n",
        "train_data = pad_sequences_pytorch(train_data, maxlen=max_len)\n",
        "test_data = pad_sequences_pytorch(test_data, maxlen=max_len)\n",
        "\n",
        "print(train_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2oJlJoQe-0L"
      },
      "source": [
        "Notice the pad sequences method worked by prepending '0's to the start of the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MvAGi0aPmsRH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     1    14    22    16    43   530\n",
            "   973  1622  1385    65   458  4468    66  3941     4   173    36   256\n",
            "     5    25   100    43   838   112    50   670     2     9    35   480\n",
            "   284     5   150     4   172   112   167     2   336   385    39     4\n",
            "   172  4536  1111    17   546    38    13   447     4   192    50    16\n",
            "     6   147  2025    19    14    22     4  1920  4613   469     4    22\n",
            "    71    87    12    16    43   530    38    76    15    13  1247     4\n",
            "    22    17   515    17    12    16   626    18 19193     5    62   386\n",
            "    12     8   316     8   106     5     4  2223  5244    16   480    66\n",
            "  3785    33     4   130    12    16    38   619     5    25   124    51\n",
            "    36   135    48    25  1415    33     6    22    12   215    28    77\n",
            "    52     5    14   407    16    82 10311     8     4   107   117  5952\n",
            "    15   256     4     2     7  3766     5   723    36    71    43   530\n",
            "   476    26   400   317    46     7     4 12118  1029    13   104    88\n",
            "     4   381    15   297    98    32  2071    56    26   141     6   194\n",
            "  7486    18     4   226    22    21   134   476    26   480     5   144\n",
            "    30  5535    18    51    36    28   224    92    25   104     4   226\n",
            "    65    16    38  1334    88    12    16   283     5    16  4472   113\n",
            "   103    32    15    16  5345    19   178    32]\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA34t8p1mu7-"
      },
      "source": [
        "# Build a Multi-Layer Perceptron\n",
        "We are now ready to build our model. We will use an Embedding layer to map from an integer that corresponds to a word, to a vector of floating point weights (the embedding). These weights are learned when we train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rl_sY4nFe-0N"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch Dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.LongTensor(data)\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IMDBDataset(train_data, train_labels)\n",
        "test_dataset = IMDBDataset(test_data, test_labels)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentClassifier(\n",
            "  (embedding): Embedding(20000, 16)\n",
            "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n",
        "        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, embedding_dim)\n",
        "        x = self.fc(x)  # (batch_size, 1)\n",
        "        x = self.sigmoid(x)\n",
        "        return x.squeeze()\n",
        "\n",
        "embedding_dimension = 16\n",
        "model = SentimentClassifier(num_words, embedding_dimension)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for data, labels in dataloader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, labels in dataloader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.6918, Train Acc: 0.5388\n",
            "Val Loss: 0.6839, Val Acc: 0.5800\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.6705, Train Acc: 0.6661\n",
            "Val Loss: 0.6536, Val Acc: 0.7440\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6258, Train Acc: 0.7678\n",
            "Val Loss: 0.5993, Val Acc: 0.7832\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5664, Train Acc: 0.8028\n",
            "Val Loss: 0.5421, Val Acc: 0.8054\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5096, Train Acc: 0.8268\n",
            "Val Loss: 0.4919, Val Acc: 0.8224\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4610, Train Acc: 0.8461\n",
            "Val Loss: 0.4507, Val Acc: 0.8408\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4201, Train Acc: 0.8597\n",
            "Val Loss: 0.4170, Val Acc: 0.8516\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3865, Train Acc: 0.8710\n",
            "Val Loss: 0.3897, Val Acc: 0.8582\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.3587, Train Acc: 0.8786\n",
            "Val Loss: 0.3674, Val Acc: 0.8660\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.3355, Train Acc: 0.8857\n",
            "Val Loss: 0.3496, Val Acc: 0.8680\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "# Split train data for validation\n",
        "val_size = int(0.2 * len(train_dataset))\n",
        "train_size = len(train_dataset) - val_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyXxmsDGe-0V"
      },
      "source": [
        "Our classifier has a validation accuracy of about 89%. Note that we make use of only the first 500 words in each review. We are also using global average pooling on our embedding before passing it to a single Dense layer, which treats each word separately without taking into consideration the ordering of the words in the sequence. To reach higher accuracy, it would be helpful to use a recurrent layer or 1D convolution which will take the sequence of the words into consideration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNJEYzWCTopy"
      },
      "source": [
        "# Visualize Embeddings with the Embedding Projector\n",
        "\n",
        "Recall the reviews are encoded as series of integers in our training data. Before we can visualize the learned embeddings, first we will need to determine which word corresponds to each number. In this case, the IMDB dataset includes a utility method `.get_word_index()` that contains a mapping from words to numbers. We will use this to build a reversed word index, which maps from numbers to words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "96uw4Szxe-0b"
      },
      "outputs": [],
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ6qkJOdfmfj"
      },
      "source": [
        "Now we can use the decode_review function to display the text for the first review. You will see padding at the beginning, since this review was shorter than our 500 word maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FZaYLC13flhp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode_review(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngIjD8HhHsS"
      },
      "source": [
        "Now that we have the number to word mapping, we are ready to retrieve the learned embedding from the model. This gives us a matrix of weights. Each row corresponds to the embedding for that number in our `reversed_word_dict` above, and the corresponding word can be found in `word_index`.\n",
        "\n",
        "In PyTorch, we can get the embedding weights directly from the embedding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N7o16O-aUlzv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000, 16)\n"
          ]
        }
      ],
      "source": [
        "# Get embedding weights\n",
        "weights = model.embedding.weight.detach().cpu().numpy()\n",
        "print(weights.shape)  # (20000, 16). Each word is mapped to an embedding vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbOD5Rv3hV1m"
      },
      "source": [
        "Next, we will format these for visualization in the embedding projector. To do so, we will need to provide two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U2q09l-8WB0j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding files saved: vecs.tsv and meta.tsv\n"
          ]
        }
      ],
      "source": [
        "out_v = open('vecs.tsv', 'w')\n",
        "out_m = open('meta.tsv', 'w')\n",
        "for word_num in range(num_words):\n",
        "    word = reverse_word_index.get(word_num, '?')\n",
        "    embeddings = weights[word_num]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "print(\"Embedding files saved: vecs.tsv and meta.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM0_cG11deiq"
      },
      "source": [
        "Now, you can open the [Embedding Projector](http://projector.tensorflow.org/) in a new window, and click on 'Load data'. Upload the `vecs.tsv` and `meta.tsv` files from above. Next, click 'Search', and type in a word to find its closest neighbors. With this small dataset, not all of the learned embeddings will be interpretable, though some will be!\n",
        "\n",
        "For example, try searching for 'beautiful'. The learned embeddings you see may be different, they depend on random weight initialization used by the model. When the author of this tutorial ran it, they saw \"loved\" and \"wonderful\" were the closest neighbors. Likewise, the closest neighbors for \"lame\" were \"awful, and poorly\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztq4QuGywNB6"
      },
      "source": [
        "# A More Advanced Model\n",
        "We will implement a more advanced model that demonstrates two things:\n",
        "1. The use of pre-trained embeddings.\n",
        "2. The use of a 1D CNN.\n",
        "\n",
        "We will be implementing a Depthwise Separable Convolutional Neural Network, which is a type of CNN that was written about in a paper published by Francois Chollet that is found here: https://arxiv.org/abs/1610.02357. Because CNN makes use of a sliding window, it will take the order of words in our text into consideration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ogIt5QWvwNB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "# download pretrained GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OzXsaycnwNB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kN266iS0wNB6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rOOsjBv5wNB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} # initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_cR62IjMwNB6"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim)) # create an array of zeros\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < num_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0xLuKJchwNB6"
      },
      "outputs": [],
      "source": [
        "# Define Separable Conv1D layers for PyTorch\n",
        "class SeparableConv1d(nn.Module):\n",
        "    \"\"\"Depthwise separable convolution\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=True):\n",
        "        super(SeparableConv1d, self).__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, \n",
        "                                   groups=in_channels, padding=padding, bias=False)\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1, bias=bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentCNN(\n",
            "  (embedding): Embedding(20000, 100)\n",
            "  (blocks): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Dropout(p=0.3, inplace=False)\n",
            "      (1): SeparableConv1d(\n",
            "        (depthwise): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), groups=100, bias=False)\n",
            "        (pointwise): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (2): ReLU()\n",
            "      (3): SeparableConv1d(\n",
            "        (depthwise): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), groups=50, bias=False)\n",
            "        (pointwise): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (4): ReLU()\n",
            "      (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1-3): 3 x Sequential(\n",
            "      (0): Dropout(p=0.3, inplace=False)\n",
            "      (1): SeparableConv1d(\n",
            "        (depthwise): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), groups=50, bias=False)\n",
            "        (pointwise): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (2): ReLU()\n",
            "      (3): SeparableConv1d(\n",
            "        (depthwise): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), groups=50, bias=False)\n",
            "        (pointwise): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (4): ReLU()\n",
            "      (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (sep_conv1): SeparableConv1d(\n",
            "    (depthwise): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), groups=50, bias=False)\n",
            "    (pointwise): Conv1d(50, 100, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (sep_conv2): SeparableConv1d(\n",
            "    (depthwise): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), groups=100, bias=False)\n",
            "    (pointwise): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (relu): ReLU()\n",
            "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the CNN model with pre-trained embeddings\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, \n",
        "                 blocks=4, filters=50, kernel_size=5, dropout_rate=0.3):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "        \n",
        "        # Embedding layer with pre-trained weights\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
        "        \n",
        "        # Build convolutional blocks\n",
        "        self.blocks = nn.ModuleList()\n",
        "        in_channels = embedding_dim\n",
        "        \n",
        "        for _ in range(blocks):\n",
        "            block = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                SeparableConv1d(in_channels, filters, kernel_size, padding=kernel_size//2),\n",
        "                nn.ReLU(),\n",
        "                SeparableConv1d(filters, filters, kernel_size, padding=kernel_size//2),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "            in_channels = filters\n",
        "        \n",
        "        # Final layers\n",
        "        self.sep_conv1 = SeparableConv1d(filters, filters * 2, kernel_size, padding=kernel_size//2)\n",
        "        self.sep_conv2 = SeparableConv1d(filters * 2, filters * 2, kernel_size, padding=kernel_size//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(filters * 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n",
        "        \n",
        "        # Apply convolutional blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        # Final convolutions\n",
        "        x = self.relu(self.sep_conv1(x))\n",
        "        x = self.relu(self.sep_conv2(x))\n",
        "        \n",
        "        # Global average pooling and classification\n",
        "        x = self.global_avg_pool(x).squeeze(-1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x.squeeze()\n",
        "\n",
        "# Create the model\n",
        "cnn_model = SentimentCNN(num_words, embedding_dim, embedding_matrix,\n",
        "                         blocks=4, filters=50, kernel_size=5, dropout_rate=0.3)\n",
        "\n",
        "# Move to device\n",
        "cnn_model = cnn_model.to(device)\n",
        "\n",
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUMkw56BwNB6"
      },
      "source": [
        "Let's compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZaYHmY1YwNB6"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss\n",
        "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "criterion_cnn = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JnZKroWvwNB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "Train Loss: 0.6933, Train Acc: 0.4979\n",
            "Val Loss: 0.6932, Val Acc: 0.4932\n",
            "\n",
            "Epoch 2/3\n",
            "Train Loss: 0.6932, Train Acc: 0.4980\n",
            "Val Loss: 0.6932, Val Acc: 0.4932\n",
            "\n",
            "Epoch 3/3\n",
            "Train Loss: 0.6931, Train Acc: 0.5008\n",
            "Val Loss: 0.6931, Val Acc: 0.5068\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the CNN model\n",
        "num_epochs_cnn = 3\n",
        "batch_size_cnn = 512\n",
        "\n",
        "# Create new dataloaders with larger batch size\n",
        "train_loader_cnn = DataLoader(train_subset, batch_size=batch_size_cnn, shuffle=True)\n",
        "val_loader_cnn = DataLoader(val_subset, batch_size=batch_size_cnn, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs_cnn):\n",
        "    train_loss, train_acc = train_epoch(cnn_model, train_loader_cnn, criterion_cnn, optimizer_cnn, device)\n",
        "    val_loss, val_acc = validate(cnn_model, val_loader_cnn, criterion_cnn, device)\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs_cnn}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhO98YHlwNB7"
      },
      "source": [
        "# Next steps\n",
        "* To learn more about Word Embeddings in PyTorch, check out the [PyTorch tutorials on embeddings](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).\n",
        "\n",
        "* [Hugging Face](https://huggingface.co/) contains large databases of pretrained embeddings and transformers you can download and reuse in your PyTorch projects."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "name": "intro_word_embeddings_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
